---
layout: archive
title: Research
permalink: /research/
author_profile: true
---


## Causal Inference under Interference using Efficient Nonparametric Estimation 
_[Causal Inference Research Lab (CIRL)](https://causal.unc.edu), UNC Chapel Hill, Jan 2022 ‑ Present_
* Developed efficient nonparametric estimation of causal network effects under interference based on semiparametric and empirical process theory.
* Used ensemble of nonparametric and ML models (spline regression, GAM, boosting, Random Forest, neural net) via SuperLearnear in R.
* Paper: Efficient Nonparametric Estimation of Stochastic Policy Effects with Clustered Interference.

## Drug Release Prediction for Reservoir‑Style Polymeric Drug Delivery Systems 
_Research Triangle Institute International, Mar 2022 ‑ Sep 2022_
* Designed and built drug release prediction model and visualized results in R, enabling rational drug implant design without extensive in vitro testing.
* Publication: Reservoir‑Style Polymeric Drug Delivery Systems: Empirical and Predictive Models for Implant Design.

## Fake News Detection using Machine Learning Methods
_Machine Learning Course Project, UNC Chapel Hill, Aug 2021 ‑ Dec 2021_
* Preprocessed fake news data based on standard NLP preprocessing procedure to generate Bag of Words, TF‑IDF, and Bigram using Pandas.
* Trained ML (SVM, Random Forest, Logistic Regression) and DL (1D CNN, BERT, LSTM, Domain Adaptation) models to build fake news detection model using scikit‑learn, PyTorch, and Tensorflow, achieved 91.4% test accuracy.

## Women’s Health Initiative Proteome‑Wide Association Study 
_[Yun Li Statistical Genetics Group](https://yunliweb.its.unc.edu), UNC Chapel Hill, Sep 2020 ‑ Aug 2021_
* Analyzed 552 protein levels of 1,002 individuals from Women’s Health Initiative data to identify protein quantitative trait loci (pQTL) using EPACTS.
* Built protein level prediction model using CV Elastic Net and investigated cardiovascular diseases related proteins based on predicted levels in R.

## Growing Student Knowledge Distillation
_Deep Learning Course Project, Seoul National University, Sep 2019 ‑ Dec 2019_
* Proposed novel knowledge distillation structure comprised of sequence of CNNs with increasing number of layers, transferring knowledge from smaller to bigger networks consecutively using PyTorch which resembles a student’s cumulative learning process.
* Achieved 89.9% test accuracy on CIFAR‑10 dataset, improvement of 0.2% test accuracy compared to baseline ResNet26.
